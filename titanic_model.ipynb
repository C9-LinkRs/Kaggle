{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load training data\ntraining_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n# Load testing data\ntesting_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# Concatenate training and testing data to split it later\nall_data = pd.concat([training_data, testing_data])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I think name prefix would be a feature\ntraining_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for nulls values\ntraining_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Ticket classes. It seems Ticket number is a kind of ID for tripulants. Due to unique tickets numbers against rows numbers, \n# i can say this ticket would be a kind of unique id.  \ntraining_data[\"Ticket\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many numeric ticked number exists. This would be a new feature, because is the best way to use Ticket column.\ntraining_data[training_data[\"Ticket\"].str.isnumeric() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Cabins per tripulant but this feature wont be used due to huge amount of NaN values\ntraining_data[\"Cabin\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print count of dead people depending on specific classes\n\nprint(pd.pivot_table(training_data, index=\"Survived\", columns=\"Sex\", values=\"Ticket\", aggfunc=\"count\"), \"\\n\")\n\nprint(pd.pivot_table(training_data, index=\"Survived\", columns=\"Pclass\", values=\"Ticket\", aggfunc=\"count\"), \"\\n\")\n\nprint(pd.pivot_table(training_data, index=\"Survived\", columns=\"Embarked\", values=\"Ticket\", aggfunc=\"count\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot histogram of Fare to observe distribution. This feature will be used, due to complement with NumericTicket\nplt.hist(x=training_data[\"Fare\"], density=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot histogram of Age to observe distribution \nplt.hist(x=training_data[\"Age\"], density=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot histogram of familiars to observe distribution. These columns means that someone is in group or alone.\nfamiliars_data = training_data[\"SibSp\"] + training_data[\"Parch\"]\nplt.hist(x=familiars_data, density=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot survived people (mean) taking into account Sex\nsns.barplot(x=training_data[\"Sex\"], y=training_data[\"Survived\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot survived (mean) people taking into account Embarked\nsns.barplot(x=training_data[\"Embarked\"], y=training_data[\"Survived\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot survived (mean) people taking into account Pclass\nsns.barplot(x=training_data[\"Pclass\"], y=training_data[\"Survived\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare training and testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ndef prepare_data(df, is_training=True):\n    # Create a new feature based on SibSp and Parch. These columns are related with if a tripulant have a group, so sum them\n    df[\"GroupSize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n\n    # Ticket is kind of ID for username but sometimes a bunch of tripulants get the same Ticket number.\n    # Create a feature taking into account if Ticket number is numeric or not\n    df[\"TicketNumeric\"] = df.apply(lambda row: 1 if row[\"Ticket\"].isnumeric() else 0, axis=1)\n\n    # Create a new Feature based on name prefix (Not considered, makes model overfits)\n    # df[\"NamePrefix\"] = df.apply(lambda row: row[\"Name\"].split(\",\")[1].split(\".\")[0].strip(), axis=1)\n    \n    # Create a new Feature based on how many cabins a tripulant has\n    df[\"CabinNumber\"] = df.apply(lambda row: 0 if pd.isna(row[\"Cabin\"]) else len(row[\"Cabin\"].split(\" \")), axis=1)\n\n    # Handle Age NaN values as filling them with median\n    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n    \n    # Handle Fare NaN values on Testing set as filling them with median\n    df[\"Fare\"].fillna(df[\"Fare\"].mean(), inplace=True)\n\n    # Drop unused columns\n    df.drop([\"SibSp\", \"Parch\", \"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n\n    # Drop NaN Embarked rows. I did it because there are only 2 rows\n    df.dropna(subset=[\"Embarked\"], inplace=True)\n    \n    # Label encoder to convert categorical data into numerical data\n    df_encoded = df.copy()\n    label_encoder = LabelEncoder()\n    df_encoded[\"Pclass\"] = label_encoder.fit_transform(df_encoded[\"Pclass\"])\n    df_encoded[\"Embarked\"] = label_encoder.fit_transform(df_encoded[\"Embarked\"])\n    df_encoded[\"Sex\"] = label_encoder.fit_transform(df_encoded[\"Sex\"])\n    \n    # Standard Scaler\n    scale = StandardScaler()\n    # df_encoded[[\"Age\", \"Fare\", \"CabinNumber\", \"GroupSize\"]] = scale.fit_transform(df_encoded[[\"Age\", \"Fare\", \"CabinNumber\", \"GroupSize\"]])\n    df_encoded[[\"Age\", \"Fare\"]] = scale.fit_transform(df_encoded[[\"Age\", \"Fare\"]])\n    \n    return df_encoded\n\n# Prepare training and testing data\ntraining_data_encoded = prepare_data(training_data.copy())\ntesting_data_encoded = prepare_data(testing_data.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(df, is_training=True):\n    if is_training:\n        # Clean Survived column\n        df[\"Survived\"] = df[\"Survived\"].astype(int)\n        \n        # Get X and Y\n        feature_columns = df.columns[1:]\n        X = df.loc[:, feature_columns].to_numpy()\n        Y = df[\"Survived\"].to_numpy()\n    else:\n        # Get X, Y is None\n        feature_columns = df.columns\n        X = df.loc[:, feature_columns].to_numpy()\n        Y = None\n    return X, Y\n\n# Split data\nX_train, Y_train = split_data(training_data_encoded)\nX_test, _ = split_data(testing_data_encoded, is_training=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, X_test.shape, Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\nTry to test different models and choose the best one."},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_valid_param = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier()\n\n# Train and Cross Validation\nknn_cross_valid = cross_val_score(knn_classifier, X_train, Y_train, cv=cross_valid_param)\n\n# Print accuracy\nprint(f\"KNN Cross Validation accuracy: {round(knn_cross_valid.mean() * 100, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nknn_classifier.fit(X_train, Y_train)\n\n# Predict output\nknn_predicted = knn_classifier.predict(X_test).astype(int)\nprint(knn_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\nsvc_classifier = SVC(probability = True, C=1, gamma=0.1, kernel= \"rbf\")\n\n# Train and Cross Validation\nsvc_cross_valid = cross_val_score(svc_classifier, X_train, Y_train, cv=cross_valid_param)\n\n# Print accuracy\nprint(f\"Linear SVM Cross Validation accuracy: {round(svc_cross_valid.mean() * 100, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nsvc_classifier.fit(X_train, Y_train)\n\n# Predict output\nsvc_predicted = svc_classifier.predict(X_test).astype(int)\nprint(svc_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multilayer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\n\nmlp_nn = MLPClassifier(solver='lbfgs', alpha=2e-5, hidden_layer_sizes=(5, 2), activation=\"relu\", random_state=1, max_iter=1269)\n\n# Train and Cross Validation\nmlp_cross_valid = cross_val_score(mlp_nn, X_train, Y_train, cv=cross_valid_param)\n\n# Print accuracy\nprint(f\"MLP Neural Network Cross Validation accuracy: {round(svc_cross_valid.mean() * 100, 2)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit model\nmlp_nn.fit(X_train, Y_train)\n\n# Predict output\nmlp_predicted = mlp_nn.predict(X_test).astype(int)\nprint(mlp_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare submission\nfinal_data = { \"PassengerId\": testing_data[\"PassengerId\"], \"Survived\": mlp_predicted }\nfinal_df = pd.DataFrame(final_data)\nfinal_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save submission\nfinal_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}